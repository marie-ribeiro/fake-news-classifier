# LSTM for sequence classification for HOMUS dataset
import numpy
import tensorflow as tf
from keras.layers import Activation
from sklearn.model_selection import train_test_split
from keras.datasets import imdb
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Conv2D
from keras.layers import LSTM
from keras.layers import MaxPooling2D
from keras.layers import Dropout
from keras.layers import Flatten
from keras.layers.embeddings import Embedding
from keras.preprocessing import sequence
from keras import optimizers
from numpy import *
from tensorflow.keras import layers
from keras.layers import TimeDistributed
from keras.utils.np_utils import to_categorical
from keras import backend as K
from keras.layers.merge import concatenate
from keras.models import Model, Sequential
from keras.layers import Dense, Input
import pandas as pd 
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.python.keras.preprocessing import sequence
from tensorflow.python.keras.preprocessing import text
from tensorflow.keras.utils import to_categorical
print(tf.version.VERSION)
#Preprocessing Data
#Parameters
#Limit on the number of features. Using the top 20,000 features
TOP_K = 20000
#Limit on the length of text sequences, longer sequences will be truncated
MAX_SEQUENCE_LENGTH = 500

#Data stats:
#Number of true entries; 21,417
#Number of false entries; 23,481
#Total entries; 44,898

csvFake = pd.read_csv('Fake.csv')
csvTrue = pd.read_csv('True.csv')
textFake_df = pd.DataFrame(csvFake[['title','text']])
textFake_df['label'] = 0  #adding label column to dataframe
textTrue_df = pd.DataFrame(csvTrue[['title','text']])
textTrue_df['label'] = 1  #adding labels

frames = [textTrue_df, textFake_df]
dataset = pd.concat(frames) #merging real and fake dataset
#This Shuffles the dataset and splits into 80% for training and 20% for testing 
#as a result, train set = 35,918 and test set = 8,980
trainTexts, testTexts = train_test_split(dataset, test_size=0.2)

def tokenize_vectorize(trainTexts, testTexts):
    #Tokenization and Vectorisation for sequence models, this method assumes that order of words is important in text, and is better for CNN and RNN
    #Create vocabulary with training texts
    tokenizer = text.Tokenizer(num_words=TOP_K)
    tokenizer.fit_on_texts(trainTexts.text)
    word_index_text = tokenizer.word_index
    #Create vocabulary with training title
    tokenizer.fit_on_texts(trainTexts.title)
    word_index_title = tokenizer.word_index    
    #Vectorize the training and validation texts
    trainSetText = tokenizer.texts_to_sequences(trainTexts.text)
    testSetText = tokenizer.texts_to_sequences(testTexts.text)
    trainSetTitle = tokenizer.texts_to_sequences(trainTexts.title)
    testSetTitle = tokenizer.texts_to_sequences(testTexts.title)    
    
    #Get max sequence length
    max_length = len(max(trainSetText, key =len))
    if max_length > MAX_SEQUENCE_LENGTH:
        max_length = MAX_SEQUENCE_LENGTH
        
    #Fix sequence length to max value. 
    #The sequence is padded in the beginning if shorter than the length
    #and longer sequences are truncated
    trainSetText = sequence.pad_sequences(trainSetText, maxlen = max_length)
    trainSetTitle = sequence.pad_sequences(trainSetTitle, maxlen = max_length)
    testSetText = sequence.pad_sequences(testSetText, maxlen = max_length)
    testSetTitle = sequence.pad_sequences(testSetTitle, maxlen = max_length)
    trainSetText = numpy.array(trainSetText)
    trainSetTitle = numpy.array(trainSetTitle)
    testSetText = numpy.array(testSetText)
    testSetTitle = numpy.array(testSetTitle)
    
    trainSetText =  trainSetText.reshape((trainSetText.shape[0], trainSetText.shape[1], 1))
    testSetText =  testSetText.reshape((testSetText.shape[0], testSetText.shape[1], 1))
    trainSetTitle = trainSetTitle.reshape((trainSetTitle.shape[0], trainSetTitle.shape[1], 1))
    testSetTitle =  testSetTitle.reshape((testSetTitle.shape[0], testSetTitle.shape[1], 1))
    #Shape should be 35918, 500, 1

    X_train = [trainSetText, trainSetTitle]
    X_test = [testSetText, testSetTitle]

    #Labels- Converting labels to binary vectors  
    Y_train = to_categorical(trainTexts.label, num_classes=2)
    Y_test = to_categorical(testTexts.label, num_classes=2)

    return X_train, X_test, Y_train, Y_test, word_index_text, word_index_title

X_train, X_test, Y_train, Y_test, word_index_text, word_index_title = tokenize_vectorize(trainTexts, testTexts)

#Network
def networkForArticalText():
    global MAX_SEQUENCE_LENGTH
    #Takes genre of article and adds to first model
    model1_in = Input(shape=(MAX_SEQUENCE_LENGTH, 1))
    model1_out = Dense(2, activation='relu')(model1_in)
    model1 = Model(model1_in, model1_out)
    return(model1)

def networkForTitle():
    #Takes titles and adds to second model
    global MAX_SEQUENCE_LENGTH
    model2_in = Input(shape=(MAX_SEQUENCE_LENGTH, 1))
    model2_out = LSTM(2, activation='relu',
                      return_sequences=True, stateful=False, name='layer2Title')(model2_in)
    model2 = Model(model2_in, model2_out)
    return(model2)

def networksCombined(model1, model2):
    #Combines 2 models and gives output
    concatenated = concatenate([model1.output, model2.output])
    concatenated = Flatten()(concatenated)
    out = Dense(2, activation='softmax', name='output')(concatenated)
    model = Model([model1.input, model2.input], outputs = out)
    return(model)
    
model1 = networkForArticalText()
model2 = networkForTitle()
model = networksCombined(model1, model2)
model.compile(loss='categorical_crossentropy',
              optimizer='Adam',
              metrics=['accuracy']) 
model.fit(
    X_train,
    Y_train,
    epochs = 10, validation_split=0.1,
    batch_size = 1,
    verbose = 2 #Removes warnings
)

#Testing
scores = model.evaluate(X_test, Y_test, batch_size = 1, verbose=2)
print("Accuracy: %.2f%%" % (scores[1]*100), flush=True)
    
model_json = model.to_json()
with open("classifierFakeRealNews.json", "w") as json_file:
    json_file.write(model_json)
model.save_weights("classifierFakeRealNews.h5")
print("Saved model to disk", flush=True)
#Once complete: make java app for use
